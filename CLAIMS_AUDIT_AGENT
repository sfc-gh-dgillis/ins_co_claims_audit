```json
{
  "agent_spec": {
    "models": {
      "orchestration": "auto"
    },
    "instructions": {
      "orchestration": "-If the user asks about claim completeness, deem a claim as complete if the following are available: Claim level data, claim lines, financial, claim notes",
      "sample_questions": [
        {
          "question": "Based on the state of new jersey's insurance claims guidelines, have any of my claims been outside of the mandated settlement window?"
        },
        {
          "question": "Was there a reserve rationale in the file notes?"
        },
        {
          "question": "Was a payment made in excess of the reserve amount for claim 1899?"
        },
        {
          "question": "Can you transcribe the media file 'consultation_5_mix_es_en.wav stored in '@ins_co.loss_claims.loss_evidence'?"
        },
        {
          "question": "What is the callers intent?"
        },
        {
          "question": "What is the customers reason for calling?"
        },
        {
          "question": "Can you give me a summary of 1899_claim_evidence1.jpeg image please?"
        },
        {
          "question": "What is the similarity score between the summary of the claim evidence and the claim description for claim 1899?"
        },
        {
          "question": "Does the file Gemini_Generated3.jpeg appear to be tampered with?"
        },
        {
          "question": "Is claim 1899 complete?"
        }
      ]
    },
    "tools": [
      {
        "tool_spec": {
          "type": "cortex_analyst_text_to_sql",
          "name": "CA_INS",
          "description": "AUTHORIZATION:\n- Database: INS_CO, Schema: LOSS_CLAIMS\n- This table manages authorization limits for performers/providers in the insurance claims system. It defines spending authority ranges with minimum and maximum amounts that can be authorized by specific performers.\n- The table establishes financial controls by setting authorization boundaries, ensuring that claim payments stay within approved limits for each performer.\n- LIST OF COLUMNS: PERFORMER_ID (unique identifier for performer - links to PERFORMER_ID in CLAIM_LINES), CURRENCY (transaction currency), FROM_AMT (minimum authorization amount), TO_AMT (maximum authorization amount)\n\nCLAIMS:\n- Database: INS_CO, Schema: LOSS_CLAIMS\n- This is the main claims table containing comprehensive information about insurance claims including policy details, loss information, and claim status. It serves as the central hub for claim management with details about when losses occurred, were reported, and processed.\n- The table tracks the complete lifecycle of claims from initial loss occurrence through reporting and processing, providing essential data for claim analysis and management.\n- LIST OF COLUMNS: CLAIM_NO (unique claim identifier), LINE_OF_BUSINESS (business type), CLAIM_STATUS (current claim state), CAUSE_OF_LOSS (loss reason), CLAIMANT_ID (person submitting claim), PERFORMER (service provider - links to PERFORMER_ID in other tables), POLICY_NO (insurance policy identifier), LOSS_DESCRIPTION (damage details), LOSS_STATE (loss location state), LOSS_ZIP_CODE (loss location zip), CREATED_DATE (claim creation date), LOSS_DATE (when loss occurred), REPORTED_DATE (when claim was reported), FNOL_COMPLETION_DATE (first notice of loss completion)\n\nCLAIM_LINES:\n- Database: INS_CO, Schema: LOSS_CLAIMS\n- This table contains individual line items for each claim, breaking down claims into specific components or damages. Each line represents a separate aspect of the overall claim with its own status and performer assignment.\n- The table enables detailed tracking of claim components, allowing for granular management of different types of damages or services within a single claim.\n- LIST OF COLUMNS: CLAIM_NO (links to CLAIM_NO in CLAIMS), LOSS_DESCRIPTION (specific line item damage), CLAIM_STATUS (line item status), CLAIMANT_ID (claim submitter), PERFORMER_ID (assigned service provider - links to AUTHORIZATION), LINE_NO (unique line identifier - links to FINANCIAL_TRANSACTIONS and INVOICES), CREATED_DATE (line creation date), REPORTED_DATE (line reporting date)\n\nFINANCIAL_TRANSACTIONS:\n- Database: INS_CO, Schema: LOSS_CLAIMS\n- This table records all financial activities related to claims including payments and reserves. It tracks the monetary flow for each claim line item with transaction types, amounts, and posting dates.\n- The table provides complete financial audit trail for claims processing, enabling tracking of reserves set aside and actual payments made for claim resolution.\n- LIST OF COLUMNS: FXID (foreign exchange transaction ID), FINANCIAL_TYPE (transaction category like RSV/PAY), CURRENCY (transaction currency), LINE_NO (links to CLAIM_LINES and INVOICES), FIN_TX_POST_DT (transaction posting date), FIN_TX_AMT (transaction amount)\n\nINVOICES:\n- Database: INS_CO, Schema: LOSS_CLAIMS\n- This table contains invoice information from vendors providing services or materials for claim repairs. It includes detailed line items with descriptions, amounts, and vendor information for tracking claim-related expenses.\n- The table facilitates vendor payment processing and expense tracking by maintaining detailed records of all invoiced items and their associated costs.\n- LIST OF COLUMNS: INV_ID (invoice identifier), INV_LINE_NBR (invoice line number), LINE_NO (links to CLAIM_LINES and FINANCIAL_TRANSACTIONS), DESCRIPTION (item/service description), CURRENCY (invoice currency), VENDOR (supplier name), INVOICE_DATE (invoice issue date), INVOICE_AMOUNT (invoice total)\n\nGUIDELINES_CHUNK_TABLE:\n- Database: INS_CO, Schema: LOSS_CLAIMS\n- This table stores processed guidelines documents in chunks for easy retrieval and reference. It contains insurance claims processing guidelines broken into manageable text segments.\n- The table supports compliance and procedural guidance by providing searchable access to regulatory and company guidelines for claims handling.\n- LIST OF COLUMNS: FILENAME (guideline document name), FILE_URL (document storage location), CHUNK (guideline text segment), LANGUAGE (content language)\n\nNOTES_CHUNK_TABLE:\n- Database: INS_CO, Schema: LOSS_CLAIMS\n- This table contains claim-specific notes and documentation broken into text chunks for analysis and retrieval. It stores detailed notes about claim progress, decisions, and observations.\n- The table provides comprehensive claim documentation history, enabling detailed tracking of claim handling decisions and progress updates.\n- LIST OF COLUMNS: FILENAME (notes document name), FILE_URL (document storage location), CHUNK (notes text segment), LANGUAGE (content language), CLAIM_NO (links to CLAIMS table)\n\nPARSED_INVOICES:\n- Database: INS_CO, Schema: LOSS_CLAIMS\n- This table contains extracted content from invoice images or documents that have been processed through parsing technology. It stores the raw extracted text from invoice files for further processing.\n- The table enables automated invoice processing by capturing and storing parsed invoice content for integration with the structured invoice data.\n- LIST OF COLUMNS: FILENAME (source invoice file), EXTRACTED_CONTENT (parsed invoice text), PARSE_DATE (when parsing occurred)\n\nREASONING:\nThis semantic model represents a comprehensive insurance claims management system that tracks the complete lifecycle of property insurance claims from initial loss through financial settlement. The model centers around claims and their associated line items, with strong relationships connecting authorization limits, financial transactions, invoices, and supporting documentation. The system enforces financial controls through performer authorization limits while maintaining detailed audit trails of all financial activities and supporting documentation.\n\nDESCRIPTION:\nThe CA_INS_CO semantic model is a comprehensive insurance claims management system from the INS_CO database's LOSS_CLAIMS schema that tracks property insurance claims from loss occurrence through financial settlement. The model centers on the CLAIMS table which connects to CLAIM_LINES for detailed damage breakdowns, with each line item linked to FINANCIAL_TRANSACTIONS for payment tracking and INVOICES for vendor billing. The system includes financial controls through the AUTHORIZATION table that sets spending limits for performers, while NOTES_CHUNK_TABLE and GUIDELINES_CHUNK_TABLE provide supporting documentation and regulatory guidance. The PARSED_INVOICES table enables automated processing of invoice documents, creating a complete end-to-end claims processing workflow with full audit trails and compliance tracking."
        }
      },
      {
        "tool_spec": {
          "type": "cortex_search",
          "name": "Guidelines",
          "description": ""
        }
      },
      {
        "tool_spec": {
          "type": "cortex_search",
          "name": "claim_notes",
          "description": ""
        }
      },
      {
        "tool_spec": {
          "type": "generic",
          "name": "CLASSIFY_FUNCTION",
          "description": "PROCEDURE/FUNCTION DETAILS:\n- Type: Custom Function\n- Language: SQL\n- Signature: (FILE_NAME VARCHAR, STAGE_NAME VARCHAR)\n- Returns: OBJECT\n- Execution: Caller context with standard null handling\n- Volatility: Volatile (uses AI processing and current timestamp)\n- Primary Function: AI-powered document classification and analysis\n- Target: Files stored in Snowflake stages\n- Error Handling: Returns structured object with success indicators\n\nDESCRIPTION:\nThis AI-powered document classification function analyzes files stored in Snowflake stages to automatically identify and categorize document types such as invoices, medical bills, insurance claims, policy documents, and other business-critical documents. The function leverages Snowflake's AI_EXTRACT capability to perform intelligent document analysis, returning a comprehensive JSON object that includes the classification type, detailed description, business context, document purpose, and a confidence score averaged across multiple AI analysis dimensions. Users should ensure they have appropriate permissions to access the specified stage and file, as the function requires read access to stage data and AI processing capabilities enabled in their Snowflake environment. The function is particularly valuable for organizations processing large volumes of mixed document types, as it provides consistent, automated classification with detailed metadata that can drive downstream business processes. The returned object structure makes it easy to integrate with data pipelines, reporting systems, and workflow automation tools while maintaining full traceability through timestamp tracking and complete classification data preservation.\n\nUSAGE SCENARIOS:\n- Document intake processing: Automatically classify incoming documents from various sources (email attachments, file uploads, scanned documents) to route them to appropriate business processes and departments\n- Insurance claims processing: Analyze uploaded claim documents, evidence images, and supporting materials to streamline claim review workflows and ensure proper categorization for regulatory compliance\n- Financial document management: Classify invoices, receipts, financial statements, and correspondence to support automated accounting processes, audit trails, and regulatory reporting requirements"
        }
      },
      {
        "tool_spec": {
          "type": "generic",
          "name": "Parse_document",
          "description": "PROCEDURE/FUNCTION DETAILS:\n- Type: Custom Function\n- Language: SQL\n- Signature: (FILE_NAME VARCHAR)\n- Returns: VARIANT\n- Execution: Caller context with standard null handling\n- Volatility: Stable (depends on file content)\n- Primary Function: Document parsing and content extraction\n- Target: Files stored in the loss_evidence stage within the ins_co.loss_claims schema\n- Error Handling: Relies on Snowflake's AI_PARSE_DOCUMENT built-in error handling\n\nDESCRIPTION:\nThis custom SQL function serves as a specialized document processing tool designed specifically for insurance loss claims operations, leveraging Snowflake's AI-powered document parsing capabilities to extract structured data from evidence files. The function takes a file name as input and automatically retrieves the corresponding document from the designated loss_evidence file stage, then processes it using advanced layout analysis with page-splitting enabled to maintain document structure integrity. This function is particularly valuable for insurance companies and claims processors who need to systematically extract information from various types of loss evidence documents such as police reports, medical records, repair estimates, or photographic evidence submitted as part of insurance claims. The function returns data in VARIANT format, providing flexibility to handle diverse document types and extracted content structures, making it ideal for downstream processing workflows that require structured data analysis. Users should ensure they have appropriate access permissions to both the file stage and the AI_PARSE_DOCUMENT functionality, and should be prepared to handle potential parsing errors for corrupted or unsupported file formats.\n\nUSAGE SCENARIOS:\n- Claims Processing Automation: Automatically extract key information from newly submitted claim evidence documents to populate claim databases and accelerate adjuster review processes\n- Bulk Document Analysis: Process large volumes of historical claim documents to extract patterns, identify fraud indicators, or perform compliance audits across the insurance portfolio\n- Integration Testing: Validate document parsing workflows in development environments by testing various document formats and structures before deploying to production claim processing systems",
          "input_schema": {
            "type": "object",
            "properties": {
              "file_name": {
                "type": "string"
              }
            },
            "required": [
              "file_name"
            ]
          }
        }
      },
      {
        "tool_spec": {
          "type": "generic",
          "name": "Image_summary",
          "description": "PROCEDURE/FUNCTION DETAILS:\n- Type: Custom Function\n- Language: SQL\n- Signature: (IMAGE_FILE VARCHAR, STAGE_NAME VARCHAR)\n- Returns: VARCHAR\n- Execution: Caller context with standard null handling\n- Volatility: Volatile (depends on external AI service)\n- Primary Function: AI-powered image analysis and summarization\n- Target: Image files stored in Snowflake stages\n- Error Handling: Relies on Snowflake Cortex error handling\n\nDESCRIPTION:\nThis custom function leverages Snowflake's Cortex AI capabilities to automatically analyze and summarize images stored in your data warehouse stages. The function takes an image file name and stage location as inputs, then uses Claude-3.5-Sonnet AI model to generate concise 100-word summaries of key insights found in the image. This is particularly valuable for organizations dealing with large volumes of visual data such as charts, diagrams, documents, or photographs that need to be catalogued and understood at scale. Users must have appropriate permissions to access both the specified stage and Snowflake Cortex services, and should be aware that processing costs will apply for each AI model invocation. The function returns a text summary that can be stored, indexed, or used for further analysis workflows.\n\nUSAGE SCENARIOS:\n- Business Intelligence: Automatically summarize chart images, dashboard screenshots, or report visualizations to create searchable metadata and improve data discovery across your organization\n- Document Processing: Extract key insights from scanned documents, forms, or technical diagrams stored in your data lake to enable automated content classification and retrieval\n- Quality Assurance: Analyze product images, inspection photos, or monitoring screenshots to generate standardized descriptions for compliance reporting and audit trails",
          "input_schema": {
            "type": "object",
            "properties": {
              "image_file": {
                "type": "string"
              },
              "stage_name": {
                "description": "default the stage to INS_CO.LOSS_CLAIMS.LOSS_EVIDENCE",
                "type": "string"
              }
            },
            "required": [
              "image_file",
              "stage_name"
            ]
          }
        }
      },
      {
        "tool_spec": {
          "type": "generic",
          "name": "TRANSCRIBE_CALLS",
          "description": "PROCEDURE/FUNCTION DETAILS:\n- Type: Custom Function\n- Language: SQL\n- Signature: (FILE_NAME VARCHAR, STAGE_NAME VARCHAR)\n- Returns: OBJECT\n- Execution: OWNER with exception handling\n- Volatility: Stable\n- Primary Function: Audio/Video File Transcription\n- Target: Media files stored in Snowflake stages\n- Error Handling: Comprehensive try-catch with structured error responses\n\nDESCRIPTION:\nThis custom function provides automated transcription capabilities for audio and video files stored in Snowflake stages, leveraging Snowflake's AI_TRANSCRIBE functionality with speaker-level timestamp granularity. The function takes a file name and stage name as parameters, processes the media file through Snowflake's AI transcription service, and returns a structured JSON object containing either the successful transcription results or detailed error information. It executes with OWNER privileges to ensure proper access to stage files and AI services, while implementing robust error handling that captures SQL error codes and messages for troubleshooting. The function is designed for business users who need to convert speech content from recorded meetings, interviews, or other audio/video materials into searchable text format. Users should ensure they have appropriate permissions to access the specified stage and that the target files are in supported audio/video formats, as the function will return detailed error information if transcription fails due to file format issues, permission problems, or AI service limitations.\n\nUSAGE SCENARIOS:\n- Meeting transcription: Convert recorded business meetings, conference calls, or interviews stored in Snowflake stages into searchable text with speaker identification and timestamps\n- Content analysis workflows: Process large volumes of audio/video content for compliance monitoring, sentiment analysis, or content categorization in data pipeline operations\n- Development and testing: Validate transcription accuracy and error handling behavior when building applications that integrate speech-to-text functionality with Snowflake's AI capabilities",
          "input_schema": {
            "type": "object",
            "properties": {
              "file_name": {
                "type": "string"
              },
              "stage_name": {
                "description": "default the stage to INS_CO.LOSS_CLAIMS.LOSS_EVIDENCE",
                "type": "string"
              }
            },
            "required": [
              "file_name",
              "stage_name"
            ]
          }
        }
      }
    ],
    "tool_resources": {
      "CA_INS": {
        "semantic_model_file": "@INS_CO.LOSS_CLAIMS.MODELS/CA_INS_CO.yaml"
      },
      "CLASSIFY_FUNCTION": {
        "execution_environment": {
          "query_timeout": 30,
          "type": "warehouse",
          "warehouse": ""
        },
        "identifier": "INS_CO_DB.ANALYTICS.CLASSIFY_DOCUMENT",
        "name": "CLASSIFY_DOCUMENT(VARCHAR, DEFAULT VARCHAR)",
        "type": "function"
      },
      "Guidelines": {
        "max_results": 4,
        "name": "INS_CO.LOSS_CLAIMS.INS_CO_GUIDELINES"
      },
      "Image_summary": {
        "execution_environment": {
          "query_timeout": 60,
          "type": "warehouse",
          "warehouse": ""
        },
        "identifier": "INS_CO.LOSS_CLAIMS.GET_IMAGE_SUMMARY",
        "name": "GET_IMAGE_SUMMARY(VARCHAR, VARCHAR)",
        "type": "function"
      },
      "Parse_document": {
        "execution_environment": {
          "query_timeout": 30,
          "type": "warehouse",
          "warehouse": ""
        },
        "identifier": "INS_CO.LOSS_CLAIMS.PARSE_DOCUMENT_FROM_STAGE",
        "name": "PARSE_DOCUMENT_FROM_STAGE(VARCHAR)",
        "type": "function"
      },
      "TRANSCRIBE_CALLS": {
        "execution_environment": {
          "query_timeout": 60,
          "type": "warehouse",
          "warehouse": ""
        },
        "identifier": "INS_CO.LOSS_CLAIMS.TRANSCRIBE_AUDIO_SIMPLE",
        "name": "TRANSCRIBE_AUDIO_SIMPLE(VARCHAR, DEFAULT VARCHAR)",
        "type": "procedure"
      },
      "claim_notes": {
        "max_results": 4,
        "name": "INS_CO.LOSS_CLAIMS.INS_CO_CLAIM_NOTES"
      }
    }
  },
  "name": "CLAIMS_AUDIT_AGENT",
  "database_name": "SNOWFLAKE_INTELLIGENCE",
  "schema_name": "AGENTS",
  "owner": "SNOWFLAKE_INTELLIGENCE_ADMIN_RL",
  "created_on": "2025-09-12T19:15:07.265+00:00"
}
```
